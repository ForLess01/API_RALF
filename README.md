# API_RALF ü§ñ

API de chat unificada que implementa balanceo de carga round-robin entre m√∫ltiples proveedores de IA (Groq, Cerebras y Google Gemini), construida con Bun.

> üåê **En producci√≥n**: Este proyecto est√° desplegado y funcionando en un VPS usando Coolify.

## üöÄ Caracter√≠sticas

- ‚úÖ **Balanceo de carga autom√°tico**: Distribuye las peticiones entre 3 proveedores de IA
- ‚úÖ **Streaming en tiempo real**: Respuestas en streaming mediante Server-Sent Events
- ‚úÖ **API REST simple**: Un solo endpoint `/chat` compatible con formato OpenAI
- ‚úÖ **TypeScript**: Totalmente tipado para mayor seguridad
- ‚úÖ **Alta velocidad**: Construido con Bun para m√°ximo rendimiento
- ‚úÖ **M√∫ltiples modelos**:
  - Groq: `moonshotai/kimi-k2-instruct-0905`
  - Cerebras: `zai-glm-4.6`
  - Google Gemini: `gemini-3-flash-preview`

## üìã Requisitos Previos

- [Bun](https://bun.sh) >= 1.0
- Claves API de los proveedores (al menos una):
  - [Groq API Key](https://console.groq.com)
  - [Cerebras API Key](https://cloud.cerebras.ai)
  - [Google Gemini API Key](https://makersuite.google.com/app/apikey)

## üîß Instalaci√≥n

1. **Clonar el repositorio**
```bash
git clone <repository-url>
cd API_RALF
```

2. **Instalar dependencias**
```bash
bun install
```

3. **Configurar variables de entorno**

Crea un archivo `.env` en la ra√≠z del proyecto:

```env
GROQ_API_KEY=tu_clave_groq
CEREBRAS_API_KEY=tu_clave_cerebras
GEMINI_API_KEY=tu_clave_gemini
PORT=3000  # Opcional, por defecto es 3000
```

> ‚ö†Ô∏è **IMPORTANTE**: El archivo `.env` est√° en `.gitignore` y **nunca debe ser commiteado**. Contiene informaci√≥n sensible.

## üèÉ Uso

### Modo desarrollo (con hot-reload)
```bash
bun run dev
```

### Modo producci√≥n
```bash
bun run start
```

El servidor se iniciar√° en `http://localhost:3000`

## üì° API Reference

### POST `/chat`

Env√≠a un mensaje y recibe una respuesta en streaming.

**Request Body:**
```json
{
  "messages": [
    {
      "role": "user",
      "content": "Explica qu√© es Fibonacci en JavaScript"
    }
  ]
}
```

**Roles v√°lidos:** `"system"`, `"user"`, `"assistant"`

**Response:** 
- Content-Type: `text/event-stream`
- Streaming de texto en tiempo real

**Ejemplo con curl (localhost):**
```bash
curl -X POST http://localhost:3000/chat \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {
        "role": "user",
        "content": "Hola, ¬øc√≥mo est√°s?"
      }
    ]
  }'
```

**Ejemplo con curl (producci√≥n):**
```bash
# Usando archivo JSON
echo '{
  "messages": [
    {"role": "user", "content": "Explica Fibonacci en JavaScript"}
  ]
}' > request.json

curl -X POST https://tu-dominio.com/chat \
  -H "Content-Type: application/json" \
  -d "@request.json"
```

**Ejemplo con fetch (JavaScript):**
```javascript
// Cambiar la URL seg√∫n el entorno
const API_URL = 'https://tu-dominio.com/chat'; // Producci√≥n
// const API_URL = 'http://localhost:3000/chat'; // Desarrollo

const response = await fetch(API_URL, {
  method: 'POST',
  headers: {
    'Content-Type': 'application/json',
  },
  body: JSON.stringify({
    messages: [
      { role: 'user', content: '¬øQu√© es recursi√≥n?' }
    ]
  })
});

const reader = response.body.getReader();
const decoder = new TextDecoder();

while (true) {
  const { done, value } = await reader.read();
  if (done) break;
  
  const chunk = decoder.decode(value);
  console.log(chunk); // Imprime cada fragmento de la respuesta
}
```

## üîÑ Balanceo de Carga

El sistema implementa un algoritmo **round-robin** que distribuye las peticiones secuencialmente:

```
Petici√≥n 1 ‚Üí Groq
Petici√≥n 2 ‚Üí Cerebras
Petici√≥n 3 ‚Üí Gemini
Petici√≥n 4 ‚Üí Groq
Petici√≥n 5 ‚Üí Cerebras
...
```

Esto permite:
- Distribuir la carga entre proveedores
- Evitar l√≠mites de rate-limiting de un solo proveedor
- Redundancia autom√°tica

## üèóÔ∏è Estructura del Proyecto

```
API_RALF/
‚îú‚îÄ‚îÄ index.ts              # Servidor principal y routing
‚îú‚îÄ‚îÄ types.ts              # Definiciones TypeScript
‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îú‚îÄ‚îÄ groq.ts          # Implementaci√≥n Groq
‚îÇ   ‚îú‚îÄ‚îÄ cerebras.ts      # Implementaci√≥n Cerebras
‚îÇ   ‚îî‚îÄ‚îÄ gemini.ts        # Implementaci√≥n Gemini
‚îú‚îÄ‚îÄ package.json
‚îú‚îÄ‚îÄ tsconfig.json
‚îú‚îÄ‚îÄ nixpacks.toml        # Configuraci√≥n para despliegue
‚îî‚îÄ‚îÄ .env                 # Variables de entorno (no commitear)
```

## üö¢ Deploy en Producci√≥n

### Despliegue con Coolify (VPS)

Este proyecto est√° en producci√≥n usando [Coolify](https://coolify.io) en un VPS:

**Pasos para desplegar:**

1. **Conecta tu repositorio Git** a Coolify
2. **Configura las variables de entorno** en el panel de Coolify:
   - `GROQ_API_KEY`
   - `CEREBRAS_API_KEY`
   - `GEMINI_API_KEY`
   - `PORT` (opcional, por defecto 3000)

3. **Coolify detectar√° autom√°ticamente** el `nixpacks.toml` y:
   - Instalar√° Bun
   - Ejecutar√° `bun install`
   - Iniciar√° el servidor con `bun run start`

4. **Configura el dominio** y SSL (Coolify lo hace autom√°ticamente)

**Ventajas de usar Coolify:**
- ‚úÖ Deploy autom√°tico con Git push
- ‚úÖ SSL/HTTPS autom√°tico con Let's Encrypt
- ‚úÖ Gesti√≥n de logs en tiempo real
- ‚úÖ Monitoreo de recursos
- ‚úÖ Rollback f√°cil a versiones anteriores

### Alternativas de Despliegue

**Railway:**
1. Conecta tu repositorio a [Railway](https://railway.app)
2. Configura las variables de entorno
3. Railway detectar√° autom√°ticamente `nixpacks.toml`

## üõ†Ô∏è Desarrollo

### Agregar un nuevo proveedor

1. Crear archivo en `services/nombre-proveedor.ts`:
```typescript
import type { AIService, ChatMessage } from '../types';

export const miServicio: AIService = {
  name: 'MiProveedor',
  async chat(messages: ChatMessage[]) {
    // Implementar l√≥gica de streaming
    return (async function* () {
      // yield chunks de respuesta
    })();
  }
};
```

2. Importar y agregar al array en `index.ts`:
```typescript
import { miServicio } from './services/mi-proveedor';

const services: AIService[] = [
  groqService,
  cerebrasService,
  geminiService,
  miServicio, // ‚Üê Nuevo servicio
];
```

## üìù Tipos

```typescript
interface ChatMessage {
  role: 'system' | 'user' | 'assistant';
  content: string;
}

interface AIService {
  name: string;
  chat: (messages: ChatMessage[]) => Promise<AsyncIterable<string>>;
}
```

## üêõ Troubleshooting

**Error: "Failed to parse JSON"**
- Verifica que el JSON est√© correctamente formateado
- En PowerShell, usa comillas simples para el JSON

**Error: No se encuentra el m√≥dulo**
- Ejecuta `bun install` nuevamente

**Respuestas vac√≠as**
- Verifica que las API keys sean v√°lidas
- Revisa los logs del servidor para errores espec√≠ficos

## üìÑ Licencia

Este proyecto es privado y no tiene licencia p√∫blica.

## ü§ù Contribuir

1. Fork el proyecto
2. Crea una rama para tu feature (`git checkout -b feature/AmazingFeature`)
3. Commit tus cambios (`git commit -m 'Add some AmazingFeature'`)
4. Push a la rama (`git push origin feature/AmazingFeature`)
5. Abre un Pull Request

---

Desarrollado con ‚ù§Ô∏è usando [Bun](https://bun.sh)